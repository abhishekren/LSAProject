{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis: From Autoencoders to Embeddings\n",
    "\n",
    "## Part I: The Autoencoder\n",
    "\n",
    "In this notebook, we explore using an autoencoder for Latent Semantic Analysis (LSA). The goal of LSA is to embed words from running text into a latent space in a way that captures the structure and relatedness of these words. An autoencoder is a neural network which is designed to compress arbitrary high-dimensional data. To accomplish this, we start with a layer the size of the high-dimensional data, and iteratively feed this data into layers of lower and lower dimension until we reach a low-dimensional layer. We then feed data from this low dimensional layer into layers of higher and higher dimension, increasing the dimensionality back to the original dimensionality. Then, we train the network on a loss function which seeks to minimize the difference between inputs and outputs. The network then learns to optimally compress the high dimensional data in such a way that it can be restored with maximum accuracy. \n",
    "\n",
    "<img src=\"files/autoencoder-architecture.png\">\n",
    "\n",
    "The autoencoder architecture is summarized above. Here are the specifications of the autoencoder we will use:\n",
    "\n",
    "1. We use just three layers for our autoencoder: an input layer, a hidden layer representing the latent space, and an output layer.\n",
    "2. We will not introduce any nonlinearities on the first two layers. On the third layer, we will introduce a sigmoid function.\n",
    "3. We will train with cross-entropy loss to match the sigmoid of the last layer.\n",
    "\n",
    "We have yet to see how we will turn running text into data which our autoencoder can use. The first and most obvious step is to perform a one-hot encoding of the words we see in our text corpus. We may be tempted to naively try and compress the one-hot encodings by feeding them into the autoencoder one-by-one and backpropogating any difference between outputs and inputs. This will not work very well, because autoencoders can only compress data well when the high-dimensional data lies in some (perhaps nonlinear) low-dimensional subspace. Here, we have a data point corresponding to each unit vector (because of one-hot encoding), so the autoencoder is essentially trying to compress the entire space. The result is that the space cannot be compressed any more efficiently than a random linear embedding. This is not surprising in light of the fact that we haven't incorporated any information about the words and how they are related (other than that they are all distinct).\n",
    "\n",
    "To solve this issue, we will instead train our autoencoder to output \"context words\" for a given input word. A \"context word\" $w'$ for a given input word $w$ is simply a word which appears near $w$ in the corpus. Our data matrix will simply be the identity, since we intend to train the autoencoder on one-hot encoding inputs. The target matrix has rows indexed by input and columns indexed by output, so the $(i, j)$th component of $y$ is 1 if the word $w'$ whose one-hot index is $j$ appears in within $L$ words of some appearance of $w$, the word whose one-hot index is $i$. Here $L$ is a hyperparameter, which we will refer to as the window size. Notice that the context words for a word $w$ are the words which appear within $2L$ of this word. When we construct $y$, we will do so by starting with all zeroes, indexing through the corpus, and for each center word $w$ and context word $w'$ at most $L$ indices away, we set the corresponding entry of $y$ to 1.\n",
    "\n",
    "This form of LSA, called \"Continuous Bag of Words\" (CBOW) LSA, could also be see as the following classification problem: Given a pair $(w', w)$, classify $w'$ as either a context word for $w$ or not a context word for $w$. The output of the autoencoder in index $w'$ when run on the one-hot encoding of $w$ would then be the classification \"score\" of $(w', w)$: an output of 1 would imply that $w'$ always appears within $L$ of $w$, so $w'$ is certainly a context word, and an ouptut of 0 would imply that $w'$ never appears within $L$ of $w$, so $w'$ is certainly not a context word. \n",
    "\n",
    "Later, we will see an embedding-focused approach to CBOW LSA that is rooted in this classification problem, but instead randomly samples positive and negative examples $(w', w)$ on which to train. We will even prove that if we sample every possible example in this alternative model, our results will be identical to those achieved via the autoencoder. Moreover, this approach may be faster and yield better results when we reduce the number of negative examples to be balanced with the number of positive examples. Reducing the number of examples saves training time, and decreasing class imbalance typically improves performance. But first, let's implement the autoencoder and see it in action on a corpus of Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import sample\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from gensim.corpora import WikiCorpus\n",
    "from nltk.corpus import reuters\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    \"\"\"A class representing an autoencoder for LSA Embedding\n",
    "\n",
    "    :param latent_dim: The dimension of the latent space in which to embed words.\n",
    "    :param corpus: A list containing all the words we wish to embed.\n",
    "    :param window: The length of the context word window.\n",
    "    :param load_weights: Whether or not to load a pretrained model we have saved\n",
    "                            from a previous training session. If a pretrained\n",
    "                            model is loaded then the fit function is disabled\n",
    "                            since our model will already be fit to the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, corpus, window=3, load_weights=False):\n",
    "\n",
    "        # Load weights if necessary\n",
    "        if load_weights:\n",
    "            self.load()\n",
    "            self.has_data = False\n",
    "            return\n",
    "        self.has_data = True\n",
    "\n",
    "        # Define activation\n",
    "        self.sigmoid = lambda x: np.divide(1, np.add(1, np.exp(-x)))\n",
    "\n",
    "        # Separate unique words and find one-hot encoding indices\n",
    "        self.words = set(corpus)\n",
    "        self.idx = {word: i for i, word in enumerate(self.words)}\n",
    "        print(f\"found words {len(self.words)} words in a corpus of length {len(corpus)}\")\n",
    "\n",
    "        # Initialize data to all negative examples\n",
    "        self.X = np.eye(len(self.words))\n",
    "        ################## BEGIN PART (A) ##################\n",
    "        # Initialize self.y to contain all negative examples\n",
    "\n",
    "        ################### END PART (A) ###################\n",
    "\n",
    "        # Add in all positive examples\n",
    "        for i in range(len(corpus)):\n",
    "            # Get the one-hot encoding indices of all words in window\n",
    "            # Remove one-hot encoding index of center word, store separately\n",
    "            start = max(0, i - window)\n",
    "            end = min(len(corpus), i + window)\n",
    "            window_indices = [self.idx[x] for x in corpus[start:end]]\n",
    "            center_index = window_indices.pop(i - start)\n",
    "            ################## BEGIN PART (B) ##################\n",
    "            # Add 1 to self. y in relevant position for each positive example \n",
    "            # (w', w), where w is the center and w' in window\n",
    "            # REMEMBER: first index of y corresponds to w', second index to w\n",
    "\n",
    "            ################### END PART (B) ###################\n",
    "\n",
    "        ################## BEGIN PART (C) ##################\n",
    "        # Normalize target so that y[w, w'] is the probability\n",
    "        # that a randomly chosen context word of w is w'\n",
    "        # (the sum of y[w, w'] over w' should be 1 for all w)\n",
    "\n",
    "        ################### END PART (C) ###################\n",
    "\n",
    "        # Randomly initialize embeddings\n",
    "        self.weights1 = np.random.rand(latent_dim, len(self.words))\n",
    "        self.weights2 = np.random.rand(len(self.words), latent_dim)\n",
    "\n",
    "    def fit(self, lr, epochs):\n",
    "        if not self.has_data: #Don't train if this is a pre-trained model!\n",
    "            return\n",
    "        for i in range(epochs):\n",
    "            data_bar = tqdm(range(len(self.words)), position=0, leave=True)\n",
    "            data_bar.set_description(f\"Processing epoch {i+1} out of {epochs}\")\n",
    "            for j in data_bar:\n",
    "                hidden = self.weights1 @ self.X[:, j]\n",
    "                output = self.sigmoid(self.weights2 @ hidden)\n",
    "                ################## BEGIN PART (D) ##################\n",
    "                # Implement Backpropagation\n",
    "\n",
    "                ################### END PART (D) ###################\n",
    "        return self\n",
    "\n",
    "    def _get_one_hot(self, word):\n",
    "        one_hot = np.zeros(len(self.words))\n",
    "        ################## BEGIN PART (E) ##################\n",
    "        # Set relevant index of one_hot to 1\n",
    "        # Hint: See __init__ code for one-hot indices\n",
    "\n",
    "        ################### END PART (E) ###################\n",
    "        return one_hot\n",
    "\n",
    "    # The reasons behind this naming convention will be apparant shortly\n",
    "    def word2vec(self, word):\n",
    "        ################## BEGIN PART (F) ##################\n",
    "        # Return the embedding (hint: use the first layer)\n",
    "\n",
    "        ################### END PART (F) ###################\n",
    "        \n",
    "    def cos_similarity(self, word1, word2):\n",
    "        embedding1 = self.word2vec(word1)\n",
    "        embedding2 = self.word2vec(word2)\n",
    "        ################## BEGIN PART (G) ##################\n",
    "        # Return the cosine of the angle between the embeddings\n",
    "\n",
    "        ################### END PART (G) ###################\n",
    "\n",
    "    def most_similar(self, word):\n",
    "        ################## BEGIN PART (H) ##################\n",
    "        # Return a list of the form [(word1, cos), (word2, cos), ...]\n",
    "        # where word1, word2, etc are other words and cos1, cos2, etc\n",
    "        # are the cos similarities with the input word\n",
    "        # Sort the list in order of decreasing similarity (so the most\n",
    "        # similar words are at the front)\n",
    "\n",
    "        ################### END PART (H) ###################\n",
    "\n",
    "    def save(self):\n",
    "        kwargs = {'protocol': pickle.HIGHEST_PROTOCOL}\n",
    "        np.save(\"weights1\", self.weights1)\n",
    "        np.save(\"weights2\", self.weights2)\n",
    "        with open('word_indices.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.idx, handle, **kwargs)\n",
    "\n",
    "    def load(self):\n",
    "        self.weights1 = np.load(\"weights1\")\n",
    "        self.weights2 = np.load(\"weights1\")\n",
    "        with open('word_indices.pickle', 'rb') as handle:\n",
    "            self.idx = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've completed the code in the cell above, run the following cell to train our autoencoder on a text corpus derived from wikipedia articles. We will only train our autoencoder on the first $N$ characters of this corpus for some large $N$, since attempting to train on the entire corpus would be very computationally involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters, feel free to tune these\n",
    "latent_space_dim = 300       # Dimension of Latent Space\n",
    "learning_rate = 0.001        # Learning rate for SGD\n",
    "num_epochs = 3               # Number of SGD epochs\n",
    "window_length = 3            # Window length for positive examples\n",
    "num_chars = 1000000          # Number of characters to read from corpus\n",
    "test_word = 'philosopher'    # Word to use in order to test results\n",
    "\n",
    "text_file = r\"wiki_en.txt\"\n",
    "file = open(text_file, 'r')\n",
    "corpus = file.read(num_chars)\n",
    "corpus = re.sub(r'/\\'', ' ', corpus)\n",
    "corpus = re.sub(r'[^\\w\\s]', '', corpus)\n",
    "corpus = re.split(r'\\s', corpus)\n",
    "corpus = list(filter(''.__ne__, corpus))\n",
    "# nltk.download('stopwords')\n",
    "# READ ME!!! If the next line of code throws an error, uncomment the code above and rerun\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "corpus = [word for word in corpus if word not in stops]\n",
    "file.close()\n",
    "embed = Embedding(latent_space_dim, corpus, window=window_length).fit(learning_rate, num_epochs)\n",
    "# embed.save()\n",
    "print(f\"Context words for '{test_word}': {embed.most_similar(test_word)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to run further tests on saved weights here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the quality of the results you saw above. Try tuning hyperparameters and retraining if the results are low-quality. Note that the Embedding class allows users to save weights and load pre-trained weights, so once a model is trained in the upper cell, you can instantiate a new embedding with pre-trained weights in the lower cell to experiment further (even if you somehow destroyed the original Embedding object from the upper cell). If you're able to obtain an embedding you believe captures relevant information about the words in the corpus and how they are related, explain why you believe so below. If not, explain why you don't believe the embedding is working well.\n",
    "##### start answer 1#####\n",
    "\n",
    "##### end answer 1#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: The Embeddings\n",
    "\n",
    "Now we elaborate upon the embedding-focused approach to Latent Semantic Analysis. In order to derive this approach, we need to name some elements of our autoencoder. For any input vector $\\mathbf{x}$, let $\\mathbf{\\phi}(\\mathbf{x})$ be the resulting hidden layer activations. Note that $\\mathbf{\\phi}(\\cdot)$ is a linear transformation from one-hot space into latent space since our hidden layer had no nonlinearity. Similarly, for any hidden layer activation vector $\\mathbf{h}$, let $\\mathbf{\\psi}^T(\\mathbf{h})$ be the pre-nonlinearity activations of the final layer; as the notation suggests, $\\mathbf{\\psi}^T(\\cdot)$ is then a linear transformation from latent space to one-hot space, and thus $\\mathbf{\\psi}(\\cdot)$ is a linear transformation from one-hot space to latent space (or more formally, their respective dual spaces, though this is irrelevant to what follows). \n",
    "\n",
    "Recall that when we constructed positive examples above, the *center* word was our input word, and we hoped to retrieve relevant *context* words in the outputs. Note also that $\\mathbf{\\phi}$ embeds inputs, while $\\mathbf{\\psi}$ embeds outputs. Thus, it seems natural that $\\mathbf{\\phi}$ is a latent space embedding of center words, while $\\mathbf{\\psi}$ is a latent space embedding of context words. If a given word $w'$ is a context word for $w$, this means that when we run $w$ under the autoencoder, the component of the output corresponding to $w'$ is large. Let $\\mathbf{w}$ and $\\mathbf{w'}$ be the one-hot encodings of $w$ and $w'$ respectively. Then the output of the neural network on input $\\mathbf{w}$ is \n",
    "\n",
    "$$\\mathbf{w'}^T\\operatorname{sigmoid}\\left(\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}\\right) = \\operatorname{sigmoid}\\left(\\mathbf{w'}^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}\\right) = \\frac{1}{1 + \\exp\\left(-\\mathbf{w'}^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}\\right)}$$\n",
    "\n",
    "Now we digress slightly and shift our focus towards the classification perspective. Recall that from this perspective, the autoencoder is attempting to classify whether $w'$ is a context word for $w$ (for all possible $w'$ at once). We saw that if we run the autoencoder on the one-hot encoding of $w$, the component of the output corresponding to $w'$ represents a classification score for $(w', w)$. It's also strictly between 0 and 1, so we might be tempted to go out on a limb and interpret this as the **probability that $w'$ is a context word for $w$**. We would then have\n",
    "\n",
    "$$P(w'\\text{ is a context word for }w) = \\frac{1}{1 + \\exp\\left(-\\mathbf{w'}^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}\\right)}$$\n",
    "\n",
    "Suppose we are given a list of tuples $\\{(w'_i, w_i, t_i)\\}_{i\\leq n}$ where $t_i = 1$ if $w'_i$ is a context word for $w_i$ and $t_i = 0$ otherwise. Assuming our model is correct, the probability of these tuples is\n",
    "\n",
    "$$P(t_0, \\dots, t_n\\mid w_0, w'_0, \\dots, w_n, w'_n) = \\prod_{i\\leq n}\\left(\\frac{1}{1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}\\right)^{t_i}\\left(1 - \\frac{1}{1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}\\right)^{1 - t_i}$$\n",
    "\n",
    "One way to find a correct model is to gather some empirically valid tuples and attempt to maximize their probability under our model. Note, however, that maximizing the function above seems difficult given the intractable product and exponents. To solve this, we can take the logarithm of both sides, and since the logarithm is monotonically increasing, maximizing $\\log P$ will be equivalent to maximizing $P$:\n",
    "\n",
    "$$\\log P(t_0, \\dots, t_n\\mid w_0, w'_0, \\dots, w_n, w'_n) = \\sum_{i\\leq n}\\left[t_i\\log\\left(\\frac{1}{1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}\\right) + (1 - t_i)\\log\\left(1 - \\frac{1}{1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}\\right)\\right]$$\n",
    "\n",
    "Hmmmmm... this looks familiar. Notice that this is simply the cross-entropy loss of the autoencoder acting on our data! When we originally trained the autoencoder, we had to include every possible pair $(w', w)$ in our data, but this form of the optimization problem gives us a way to include only as many pairs as we need. As discussed before, the large majority of examples drawn from actual text corpuses are negative, leading to class imbalance and slow training. To fix these issues, we can reframe the optimization problem in terms of the embeddings $\\mathbf{\\phi}(\\mathbf{w}_i)$ and $\\mathbf{\\psi}(\\mathbf{w}_i)$ for all words $w$ in the corpus.\n",
    "\n",
    "First, we simply our expression above:\n",
    "\n",
    "\\begin{align*}\\log P(t_0, \\dots, t_n\\mid w_0, w'_0, \\dots, w_n, w'_n) &= \\sum_{i\\leq n}\\left[t_i\\log\\left(\\frac{1}{1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}\\right) + (1 - t_i)\\log\\left(\\frac{\\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}{1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)}\\right)\\right]\\\\&=\\sum_{i\\leq n}\\left[(1 - t_i)\\log\\exp\\left(-\\mathbf{\\psi}(\\mathbf{w'}_i)^T\\mathbf{\\phi}(\\mathbf{w}_i)\\right) - (1 - t_i + t_i)\\log \\left(1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)\\right)\\right]\\\\&=-\\sum_{i\\leq n}\\left[(1 - t_i)\\mathbf{\\psi}(\\mathbf{w'}_i)^T\\mathbf{\\phi}(\\mathbf{w}_i) + \\log \\left(1 + \\exp\\left(-\\mathbf{w'}_i^T\\mathbf{\\psi}^T\\mathbf{\\phi}\\mathbf{w}_i\\right)\\right)\\right]\\end{align*}\n",
    "\n",
    "With this expression in a simpler form, we can evaluate its gradient with respect to the embedding vectors:\n",
    "\n",
    "\\begin{align*}\\nabla_{\\mathbf\\phi(\\mathbf w_i)}\\log P &= -\\left[(1 - t_0)\\mathbf\\psi(\\mathbf w'_i) - \\frac{e^{-\\mathbf\\psi(\\mathbf w'_i)^T\\mathbf\\phi(\\mathbf w_i)}}{1 + e^{-\\mathbf\\psi(\\mathbf w'_i)^T\\mathbf\\phi(\\mathbf w_i)}}\\mathbf\\psi(\\mathbf w'_i)\\right]\\\\&=\\left[t_0 - \\frac{1}{1 + e^{-\\mathbf\\psi(\\mathbf w'_i)^T\\mathbf\\phi(\\mathbf w_i)}} \\right] \\mathbf\\psi(\\mathbf w'_i)\\\\\\nabla_{\\mathbf\\psi(w_i')}\\log P &= -\\left[(1 - t_0)\\mathbf\\phi(\\mathbf w'_i) - \\frac{e^{-\\mathbf\\psi(\\mathbf w'_i)^T\\mathbf\\phi(\\mathbf w_i)}}{1 + e^{-\\mathbf\\psi(\\mathbf w'_i)^T\\mathbf\\phi(\\mathbf w_i)}}\\mathbf\\phi(\\mathbf w_i)\\right]\\\\&=\\left[t_0 - \\frac{1}{1 + e^{-\\mathbf\\psi(\\mathbf w'_i)^T\\mathbf\\phi(\\mathbf w_i)}} \\right] \\mathbf\\phi(\\mathbf w_i)\\end{align*}\n",
    "\n",
    "Since we hope to maximize $\\log P$, we can produce the embeddings with gradient ascent! In this format, we also have the power to choose the examples $(w', w)$ on which we train, meaning we can avoid training on too many negative examples, and ultimately we can avoid the slow training times and class imbalance which the autoencoder suffers from. Now let's implement an embedding class which performs gradient ascent as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AltEmbedding:\n",
    "\n",
    "    def __init__(self, latent_dim, corpus, window=3):\n",
    "        \n",
    "        # Separate unique words and find one-hot encoding indices\n",
    "        self.words = set(corpus)\n",
    "        ################## BEGIN PART (I) ##################\n",
    "        # Create a one-hot encoding of the set of words\n",
    "        # HINT: how did we do this last time?\n",
    "\n",
    "        ################### END PART (I) ###################\n",
    "        self.freqs = Counter([idx[word] for word in corpus])\n",
    "        self.idx = {i: idx[word] for i, word in enumerate(corpus)}\n",
    "        print(f\"found words {len(self.words)} words in a corpus of length {len(corpus)}\")\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.center = np.random.randn(len(self.words), latent_dim)\n",
    "        self.context = np.random.randn(latent_dim, len(self.words))\n",
    "        self.window = window\n",
    "\n",
    "    def fit(self, lr, epochs):\n",
    "        for i in range(epochs):\n",
    "            data_bar = tqdm(range(len(self.words)), position=0, leave=True)\n",
    "            data_bar.set_description(f\"Processing epoch {i+1} out of {epochs}\")\n",
    "            for j in data_bar:\n",
    "                center_index = self.idx[j]\n",
    "                ################## BEGIN PART (J) ##################\n",
    "                # Find the start and end indices of the window\n",
    "                # HINT: Remember to ensure the start end end indices are\n",
    "                #       not out of range!\n",
    "\n",
    "                ################### END PART (J) ###################\n",
    "                for k in range(start, end):\n",
    "                    if k == 0: next\n",
    "                    context_index = self.idx[k]\n",
    "                    e = np.exp(-(self.center[[center_index], :] @ self.context[:, [context_index]])[0][0])\n",
    "                    ################## BEGIN PART (K) ##################\n",
    "                    # Perform Gradient Ascent updates for positive examples\n",
    "                    # HINT: We have already calcualted th exponential term for you.\n",
    "                    # HINT: See the gradient ascent updates for negative examples if\n",
    "                    #       you have trouble. Remember to adjust the coefficient based\n",
    "                    #       on the value of t_i (these are positive examples, not negative)\n",
    "\n",
    "                    ################### END PART (K) ###################\n",
    "                neg_index = choices(range(len(self.words)), weights=self.freqs, k=2*self.window)\n",
    "                for k in neg_index:\n",
    "                    context_index = k\n",
    "                    e = np.exp(-(self.center[[center_index], :] @ self.context[:, [context_index]])[0][0])\n",
    "                    self.center[[center_index], :] += lr * -1 / (1 + e) * self.context[:, [context_index]].T\n",
    "                    self.context[:, [context_index]] += lr * -1 / (1 + e) * self.center[[center_index], :].T\n",
    "                    \n",
    "    def save(self):\n",
    "        kwargs = {'protocol': pickle.HIGHEST_PROTOCOL}\n",
    "        np.save(\"center\", self.center)\n",
    "        np.save(\"context\", self.context)\n",
    "        with open('alt_word_indices.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.idx, handle, **kwargs)\n",
    "\n",
    "    def load(self):\n",
    "        self.center = np.load(\"center\")\n",
    "        self.context = np.load(\"context\")\n",
    "        with open('alt_word_indices.pickle', 'rb') as handle:\n",
    "            self.idx = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code above, run the following cell to try out your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters, feel free to tune these\n",
    "latent_space_dim = 300       # Dimension of Latent Space\n",
    "learning_rate = 0.001        # Learning rate for SGD\n",
    "num_epochs = 3               # Number of SGD epochs\n",
    "window_length = 3            # Window length for positive examples\n",
    "num_chars = 1000000          # Number of characters to read from corpus\n",
    "test_word = 'philosopher'    # Word to use in order to test results\n",
    "\n",
    "embed = AltEmbedding(latent_space_dim, corpus, window=window_length).fit(learning_rate, num_epochs)\n",
    "# embed.save()\n",
    "print(f\"Context words for '{test_word}': {embed.most_similar(test_word)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to run further tests on saved weights here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the quality of the results you saw above. Try tuning hyperparameters and retraining if the results are low-quality. Note that the Embedding class allows users to save weights and load pre-trained weights, so once a model is trained in the upper cell, you can instantiate a new embedding with pre-trained weights in the lower cell to experiment further (even if you somehow destroyed the original Embedding object from the upper cell). If you're able to obtain an embedding you believe captures relevant information about the words in the corpus and how they are related, explain why you believe so below. If not, explain why you don't believe the embedding is working well.\n",
    "##### start answer 2#####\n",
    "\n",
    "##### end answer 2#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Applications\n",
    "It turns out that the gensim package handles a lot of heavy lifting for us. Now that we have a good idea of what is going on under the hood of word2vec and Latent Semantic Analysis, we will see some implementations of such algorithms.\n",
    "a) First, we are going to see how word2vec can be used to find word similarities using gensim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def download_data_as_text():\n",
    "    for article in reuters.fileids():\n",
    "        all_articles += reuters.raw(article)\n",
    "    return all_articles\n",
    "\n",
    "# Generate the model using Word2Vec and save it in a file called reuters.word2vec\n",
    "def train_word2vec(text):\n",
    "    # Begin part A\n",
    "    \n",
    "    \n",
    "    # End part A\n",
    "\n",
    "# Load the model in reuters.word2vec to create a function that takes in a word and outputs the 10 most similar words\n",
    "# semantically to the given word.\n",
    "def word_sim(word):\n",
    "    # Begin part B\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # End part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We hope you enjoyed this notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
